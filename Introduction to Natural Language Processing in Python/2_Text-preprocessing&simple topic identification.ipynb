{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Words counts with bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Bag-of-words?\n",
    "- Is a basic method for finding topics in a text. But fisrtly, it is needed to create tokens using tokenization and then count up them all.\n",
    "- The more frequent a word, the more important it might be\n",
    "- Can be a great way to determine the significant words in a text\n",
    "\n",
    "Basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "##Using Counter class to tokens created with word_tokenize\n",
    "counter=Counter(word_tokenize(text)) ##this need pre-processing\n",
    "##result counter object similar to a dictionary\n",
    "counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##series of tuples with this structure: (holds token, represent frequency)\n",
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'''Debugging''' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\n\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Proc\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##using a Wikipedia article which was copy on a txt file\n",
    "with open('article.txt','r',encoding='UTF-8') as file:\n",
    "    article=file.read()\n",
    "article[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prevent', 'correct', 'operation', 'of', 'computer', 'software']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [word.lower() for word in tokens]\n",
    "lower_tokens[15:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 283), (',', 281), ('.', 170), ('of', 155), (\"''\", 124), ('to', 120), ('a', 110), ('``', 86), ('in', 82), ('and', 76), ('(', 75), (')', 75), ('debugging', 74), (':', 59), ('for', 50)]\n"
     ]
    }
   ],
   "source": [
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "##Article title: Debugging\n",
    "# Print the 15 most common tokens\n",
    "print(bow_simple.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Simple text preprocessing\n",
    "#### Why preprocess?\n",
    "- Helps make for better input data\n",
    "- When performing machine learning or other statistical methods\n",
    "    \n",
    "Some examples:\n",
    "- Tokenization to create a bag of words\n",
    "- Lowercasing words\n",
    "- Lemmatization/Stemming: Shorten words to their root stems\n",
    "- Removing stop words, punctuation, or unwanted tokens (examples: \"the\", \"and\", \".\", \",\" , etc)\n",
    "\n",
    "    \n",
    "<b>Recommendation:</b> Good to experiment with different approaches\n",
    "    \n",
    "Basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat is in the box. The cat likes the box. The box is over the cat.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "##text to use in this example\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'in', 'the', 'box', 'the', 'cat', 'likes', 'the', 'box', 'the', 'box', 'is', 'over', 'the', 'cat']\n"
     ]
    }
   ],
   "source": [
    "#list comprehension to tokenize sentences and also lowering the words\n",
    "##use string alpha method to only return alphabetic strings (this will effectively strip tokens with numbers or punctuation)\n",
    "tokens = [w for w in word_tokenize(text.lower()) \n",
    "                  if w.isalpha()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if stopwords wasn't used before, download it\n",
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']\n"
     ]
    }
   ],
   "source": [
    "#list comprehension to remove words that are in the stopwords list\n",
    "# the english stopwords comes built in with the NLTK library - need to be downloaded if first time using it\n",
    "\n",
    "no_stops = [t for t in tokens \n",
    "                    if t not in stopwords.words('english')]\n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##count the pre-processed words\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", \"''\", 'debugging', \"''\", \"'\", 'is', 'the', 'process', 'of', 'finding', 'and', 'resolving', 'of', 'defects', 'that', 'prevent', 'correct', 'operation', 'of', 'computer', 'software', 'or', 'a', 'system', '.', 'numerous', 'books', 'have', 'been', 'written', 'about', 'debugging', '(', 'see', 'below', ':', '#', 'further', 'reading|further', 'reading', ')', ',', 'as', 'it', 'involves', 'numerous', 'aspects', ',', 'including', 'interactive']\n"
     ]
    }
   ],
   "source": [
    "#using lowercase words from the previous exercise\n",
    "print(lower_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debugging', 'is', 'the', 'process', 'of', 'finding', 'and', 'resolving', 'of', 'defects', 'that', 'prevent', 'correct', 'operation', 'of', 'computer', 'software', 'or', 'a', 'system', 'numerous', 'books', 'have', 'been', 'written', 'about', 'debugging', 'see', 'below', 'further', 'reading', 'as', 'it', 'involves', 'numerous', 'aspects', 'including', 'interactive', 'debugging', 'control', 'flow', 'integration', 'testing', 'files', 'monitoring', 'application', 'system', 'memory', 'dumps', 'profiling']\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "print(alpha_only[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debugging', 'process', 'finding', 'resolving', 'defects', 'prevent', 'correct', 'operation', 'computer', 'software', 'system', 'numerous', 'books', 'written', 'debugging', 'see', 'reading', 'involves', 'numerous', 'aspects', 'including', 'interactive', 'debugging', 'control', 'flow', 'integration', 'testing', 'files', 'monitoring', 'application', 'system', 'memory', 'dumps', 'profiling', 'computer', 'programming', 'statistical', 'process', 'control', 'special', 'design', 'tactics', 'improve', 'detection', 'simplifying', 'changes', 'origin', 'computer', 'log', 'entry']\n"
     ]
    }
   ],
   "source": [
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "print(no_stops[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if WordNetLemmatizer wasn't used before, download it\n",
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debugging', 'process', 'finding', 'resolving', 'defect', 'prevent', 'correct', 'operation', 'computer', 'software', 'system', 'numerous', 'book', 'written', 'debugging', 'see', 'reading', 'involves', 'numerous', 'aspect', 'including', 'interactive', 'debugging', 'control', 'flow', 'integration', 'testing', 'file', 'monitoring', 'application', 'system', 'memory', 'dump', 'profiling', 'computer', 'programming', 'statistical', 'process', 'control', 'special', 'design', 'tactic', 'improve', 'detection', 'simplifying', 'change', 'origin', 'computer', 'log', 'entry']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "print(lemmatized[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 74), ('system', 47), ('bug', 31), ('software', 30), ('problem', 30), ('tool', 30), ('debugger', 26), ('process', 24), ('computer', 23), ('used', 22)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_text",
   "language": "python",
   "name": "ml_text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
