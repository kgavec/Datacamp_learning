{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Words counts with bag-of-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Bag-of-words?\n",
    "- Is a basic method for finding topics in a text. But fisrtly, it is needed to create tokens using tokenization and then count up them all.\n",
    "- The more frequent a word, the more important it might be\n",
    "- Can be a great way to determine the significant words in a text\n",
    "\n",
    "Basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'The': 3,\n",
       "         'cat': 3,\n",
       "         'is': 2,\n",
       "         'in': 1,\n",
       "         'the': 3,\n",
       "         'box': 3,\n",
       "         '.': 3,\n",
       "         'likes': 1,\n",
       "         'over': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "##Using Counter class to tokens created with word_tokenize\n",
    "counter=Counter(word_tokenize(text)) ##this need pre-processing\n",
    "##result counter object similar to a dictionary\n",
    "counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##series of tuples with this structure: (holds token, represent frequency)\n",
    "counter.most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Counter with bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'''Debugging''' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\n\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Proc\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##using a Wikipedia article which was copy on a txt file\n",
    "with open('article.txt','r',encoding='UTF-8') as file:\n",
    "    article=file.read()\n",
    "article[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prevent', 'correct', 'operation', 'of', 'computer', 'software']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [word.lower() for word in tokens]\n",
    "lower_tokens[15:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 283), (',', 281), ('.', 170), ('of', 155), (\"''\", 124), ('to', 120), ('a', 110), ('``', 86), ('in', 82), ('and', 76), ('(', 75), (')', 75), ('debugging', 74), (':', 59), ('for', 50)]\n"
     ]
    }
   ],
   "source": [
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "##Article title: Debugging\n",
    "# Print the 15 most common tokens\n",
    "print(bow_simple.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Simple text preprocessing\n",
    "#### Why preprocess?\n",
    "- Helps make for better input data\n",
    "- When performing machine learning or other statistical methods\n",
    "    \n",
    "Some examples:\n",
    "- Tokenization to create a bag of words\n",
    "- Lowercasing words\n",
    "- Lemmatization/Stemming: Shorten words to their root stems\n",
    "- Removing stop words, punctuation, or unwanted tokens (examples: \"the\", \"and\", \".\", \",\" , etc)\n",
    "\n",
    "    \n",
    "<b>Recommendation:</b> Good to experiment with different approaches\n",
    "    \n",
    "Basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The cat is in the box. The cat likes the box. The box is over the cat.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "##text to use in this example\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'in', 'the', 'box', 'the', 'cat', 'likes', 'the', 'box', 'the', 'box', 'is', 'over', 'the', 'cat']\n"
     ]
    }
   ],
   "source": [
    "#list comprehension to tokenize sentences and also lowering the words\n",
    "##use string alpha method to only return alphabetic strings (this will effectively strip tokens with numbers or punctuation)\n",
    "tokens = [w for w in word_tokenize(text.lower()) \n",
    "                  if w.isalpha()]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if stopwords wasn't used before, download it\n",
    "import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'box', 'cat', 'likes', 'box', 'box', 'cat']\n"
     ]
    }
   ],
   "source": [
    "#list comprehension to remove words that are in the stopwords list\n",
    "# the english stopwords comes built in with the NLTK library - need to be downloaded if first time using it\n",
    "\n",
    "no_stops = [t for t in tokens \n",
    "                    if t not in stopwords.words('english')]\n",
    "print(no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 3), ('box', 3)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##count the pre-processed words\n",
    "Counter(no_stops).most_common(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", \"''\", 'debugging', \"''\", \"'\", 'is', 'the', 'process', 'of', 'finding', 'and', 'resolving', 'of', 'defects', 'that', 'prevent', 'correct', 'operation', 'of', 'computer', 'software', 'or', 'a', 'system', '.', 'numerous', 'books', 'have', 'been', 'written', 'about', 'debugging', '(', 'see', 'below', ':', '#', 'further', 'reading|further', 'reading', ')', ',', 'as', 'it', 'involves', 'numerous', 'aspects', ',', 'including', 'interactive']\n"
     ]
    }
   ],
   "source": [
    "#using lowercase words from the previous exercise\n",
    "print(lower_tokens[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debugging', 'is', 'the', 'process', 'of', 'finding', 'and', 'resolving', 'of', 'defects', 'that', 'prevent', 'correct', 'operation', 'of', 'computer', 'software', 'or', 'a', 'system', 'numerous', 'books', 'have', 'been', 'written', 'about', 'debugging', 'see', 'below', 'further', 'reading', 'as', 'it', 'involves', 'numerous', 'aspects', 'including', 'interactive', 'debugging', 'control', 'flow', 'integration', 'testing', 'files', 'monitoring', 'application', 'system', 'memory', 'dumps', 'profiling']\n"
     ]
    }
   ],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "print(alpha_only[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debugging', 'process', 'finding', 'resolving', 'defects', 'prevent', 'correct', 'operation', 'computer', 'software', 'system', 'numerous', 'books', 'written', 'debugging', 'see', 'reading', 'involves', 'numerous', 'aspects', 'including', 'interactive', 'debugging', 'control', 'flow', 'integration', 'testing', 'files', 'monitoring', 'application', 'system', 'memory', 'dumps', 'profiling', 'computer', 'programming', 'statistical', 'process', 'control', 'special', 'design', 'tactics', 'improve', 'detection', 'simplifying', 'changes', 'origin', 'computer', 'log', 'entry']\n"
     ]
    }
   ],
   "source": [
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
    "print(no_stops[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if WordNetLemmatizer wasn't used before, download it\n",
    "import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['debugging', 'process', 'finding', 'resolving', 'defect', 'prevent', 'correct', 'operation', 'computer', 'software', 'system', 'numerous', 'book', 'written', 'debugging', 'see', 'reading', 'involves', 'numerous', 'aspect', 'including', 'interactive', 'debugging', 'control', 'flow', 'integration', 'testing', 'file', 'monitoring', 'application', 'system', 'memory', 'dump', 'profiling', 'computer', 'programming', 'statistical', 'process', 'control', 'special', 'design', 'tactic', 'improve', 'detection', 'simplifying', 'change', 'origin', 'computer', 'log', 'entry']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "print(lemmatized[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 74), ('system', 47), ('bug', 31), ('software', 30), ('problem', 30), ('tool', 30), ('debugger', 26), ('process', 24), ('computer', 23), ('used', 22)]\n"
     ]
    }
   ],
   "source": [
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introduction to gensim\n",
    "#### What is gensim?\n",
    "- Popular open-source NLP library\n",
    "- Uses top academic models to perform complex tasks:\n",
    "    - Building document or word vectors\n",
    "    - Performing topic identification and document comparison\n",
    "\n",
    "#### What is word embedding (document vector)?\n",
    "- Is train for a larger corpus.\n",
    "- Is multidimensional representation of a word (multidimensional array).\n",
    "- With this vectors is possible to see relationships between words or documents.\n",
    "    \n",
    "Def: Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus.\n",
    "    \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/0*4ctH2ps5Y-ZIYW1g.png\" width=\"800\" height=\"400\">    \n",
    "    \n",
    "    \n",
    "Basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "my_documents = ['The movie was about a spaceship and aliens.','I really liked the movie!',\n",
    "'Awesome action scenes, but boring characters.','The movie was awful! I hate alien films.','Space is cool! I liked the movie.','More space films, please!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'movie', 'was', 'about', 'a', 'spaceship', 'and', 'aliens', '.'],\n",
       " ['i', 'really', 'liked', 'the', 'movie', '!'],\n",
       " ['awesome', 'action', 'scenes', ',', 'but', 'boring', 'characters', '.'],\n",
       " ['the', 'movie', 'was', 'awful', '!', 'i', 'hate', 'alien', 'films', '.'],\n",
       " ['space', 'is', 'cool', '!', 'i', 'liked', 'the', 'movie', '.'],\n",
       " ['more', 'space', 'films', ',', 'please', '!']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###pre-process using lowercase\n",
    "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
    "tokenized_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a gensim dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, 'a': 1, 'about': 2, 'aliens': 3, 'and': 4, 'movie': 5, 'spaceship': 6, 'the': 7, 'was': 8, '!': 9, 'i': 10, 'liked': 11, 'really': 12, ',': 13, 'action': 14, 'awesome': 15, 'boring': 16, 'but': 17, 'characters': 18, 'scenes': 19, 'alien': 20, 'awful': 21, 'films': 22, 'hate': 23, 'cool': 24, 'is': 25, 'space': 26, 'more': 27, 'please': 28}\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_docs)\n",
    "##print Diccionary of all tokens with their ID's\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a gensim corpus\n",
    "- gensim models can be easily saved, updated, and reused\n",
    "- The dictionary created can also be updated\n",
    "- This more advanced and feature rich bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In each tuple items: (token ID, token frequency in document)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
       " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
       " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
       " [(0, 1),\n",
       "  (5, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1)],\n",
       " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
       " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##This transforms each document into bag-of-words using the token ID's and its respective frequency in de document\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "#Each list inside represents a document\n",
    "#Each document becames a series of tuples\n",
    "print('In each tuple items: (token ID, token frequency in document)')\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples: Creating and querying a corpus with gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['uses', 'file', 'operating', 'system', 'place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[\"''\", 'debugging', \"''\", 'process', 'finding'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['use', 'dmy', 'dates|date=september', '2013',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['use', 'dmy', 'dates|date=march', '2014', 'in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[\"''\", 'reverse', 'engineering', \"''\", 'also',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            articles\n",
       "0  ['uses', 'file', 'operating', 'system', 'place...\n",
       "1  [\"''\", 'debugging', \"''\", 'process', 'finding'...\n",
       "2  ['use', 'dmy', 'dates|date=september', '2013',...\n",
       "3  ['use', 'dmy', 'dates|date=march', '2014', 'in...\n",
       "4  [\"''\", 'reverse', 'engineering', \"''\", 'also',..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "##using a Wikipedia article tokens which was copied on a csv file\n",
    "wiki_tokens=pd.read_csv('Wiki_articles_tokens.csv')\n",
    "print(wiki_tokens.shape)\n",
    "wiki_tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uses', 'file', 'operating', 'system', 'placement', 'software', '.svg|thumb|upright|a', 'diagram', 'showing', 'user', 'computing', '|user', 'interacts', 'application', 'software', 'typical', 'desktop', 'computer.the', 'application', 'software', 'layer', 'interfaces', 'operating', 'system', 'turn']\n"
     ]
    }
   ],
   "source": [
    "wiki_tok=[]\n",
    "for num in range(len(wiki_tokens['articles'])):\n",
    "    wiki_tok.append([text.replace(\"'\",\"\").strip().replace(\"[\",\"\").replace(\"]\",\"\") for text in wiki_tokens['articles'][num].split(',')])\n",
    "print(wiki_tok[0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer\n"
     ]
    }
   ],
   "source": [
    "# Import Dictionary\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(wiki_tok)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 88), (20, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), (57, 1), (58, 1), (59, 3)]\n"
     ]
    }
   ],
   "source": [
    "# Create a gensim MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in wiki_tok]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gensim corpus and dictionary to see the most common terms per document and across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engineering 91\n",
      "\"\" 88\n",
      "reverse 71\n",
      "software 51\n",
      "cite 26\n"
     ]
    }
   ],
   "source": [
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Python <b> defaultdict </b> and <b>itertools</b> to help with the creation of intermediate data structures for analysis. \n",
    "- <b> defaultdict : </b> allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise.\n",
    "\n",
    "- <b> itertools.chain.from_iterable() : </b>  allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Install : conda install -c conda-forge python-utils\n",
    "import collections\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a defaultdict called total_word_count in which the keys are all the token ids (word_id) and the values are the sum of \n",
    "#their occurrence across all documents (word_count).\n",
    "total_word_count = collections.defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\" 1042\n",
      "computer 594\n",
      "software 450\n",
      "`` 345\n",
      "cite 322\n"
     ]
    }
   ],
   "source": [
    "# Create a sorted list from the defaultdict: sorted_word_count\n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Tf-idf with gensim\n",
    "#### What is tf-idf?\n",
    "- Term frequency - inverse document frequency\n",
    "- Allows you to determine the most important words in each document\n",
    "- Each corpus may have shared words beyond just stopwords\n",
    "- These words should be down-weighted in importance\n",
    "- Example from astronomy: \"Sky\"\n",
    "\n",
    "#### Tf-idf formula\n",
    "\n",
    " <img src=\"https://plumbr.io/app/uploads/2016/06/tf-idf.png\" >  \n",
    "\n",
    "<b>Term frequency </b>= percentage share of the word compared to all tokens in the document \n",
    "    \n",
    "<b>Inverse document frequency</b> = logarithm of the total number of documents in a corpora divided by the number of documents containing the term\n",
    "\n",
    "Observations:\n",
    "- The WEIGHT will be low if the term doesnt appear often in the document because tf variable will then be low.\n",
    "- The WEIGHT will be low if the logarithm is close to zero, meaning the internal equation is low (remember log(1) = 0).\n",
    "    - So if the internal operation is close to 1 then the logarithm will be close to zero\n",
    "\n",
    "Tf-idf with gensim example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(24, 0.011385999579543369),\n",
       " (55, 0.04075401940563056),\n",
       " (56, 0.013668277699438486),\n",
       " (57, 0.010821777804552644),\n",
       " (63, 0.010821777804552644),\n",
       " (67, 0.017152112011352465),\n",
       " (75, 0.0336603857417326),\n",
       " (82, 0.012660668413599637),\n",
       " (94, 0.0063303342067998185),\n",
       " (98, 0.0168301928708663)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "#Bag-of-words corpus to translate it into a TF-idf model \n",
    "tfidf = TfidfModel(corpus)\n",
    "#reference each document by using it like a dictionary key with our new tf-idf model\n",
    "## tuple (token id , token weight)\n",
    "tfidf[corpus[1]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(44, 0.01862887035545794), (59, 0.020941657966372935), (64, 0.012381948203932928), (77, 0.004188331593274587), (78, 0.015923132108888866)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the tfidf weights of doc 3: tfidf_weights\n",
    "tfidf_weights = tfidf[corpus[2]]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crashes 0.5350890921415871\n",
      "segmentation 0.2283353260175823\n",
      "attempting 0.1910775853066664\n",
      "crashed 0.1646427975820268\n",
      "invalid 0.15923132108888866\n"
     ]
    }
   ],
   "source": [
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_text",
   "language": "python",
   "name": "ml_text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
