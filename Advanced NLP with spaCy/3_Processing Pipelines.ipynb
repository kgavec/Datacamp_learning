{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Processing Pipelines\n",
    "\n",
    "A series of functions applied to a Doc to add attributes like part-of-speech tags, dependency labels or named entities.\n",
    "    \n",
    "Pipeline components provided by spaCy\n",
    "\n",
    "#### What happens when you call nlp?\n",
    "    \n",
    "> doc = nlp('This is a sentence.') \n",
    "    \n",
    "- The tokenizer is applied to turn the string of text into a Doc object:\n",
    "    - A series of pipeline components is applied to the Doc in order (tagger->parser->ner) and then the process doc is returned.\n",
    "    \n",
    "<img src=\"https://d33wubrfki0l68.cloudfront.net/16b2ccafeefd6d547171afa23f9ac62f159e353d/48b91/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg\" width=\"800\" height=\"800\">    \n",
    "\n",
    "|Name| Description| Creates|Obs|\n",
    "|---|---|---|---|\n",
    "|tagger| Part-of-speech tagger| Token.tag||\n",
    "|parser| Dependency parser |Token.dep , Token.head , Doc.sents , Doc.noun_chunks| Responsible of detecting sentences and base noun phrases|\n",
    "|ner| Named entity recognizer |Doc.ents , Token.ent_iob , Token.ent_type| Adds detected entities to the doc and also indicate if a token is part of an entity or not|\n",
    "|textcat| Text classier |Doc.cats| Not included in any of the pre-trained models by default|\n",
    "    \n",
    "   \n",
    "    \n",
    "#### Under the hood\n",
    "- Pipeline dened in model's meta.json in order\n",
    "- Built-in components need binary data to make predictions\n",
    "    \n",
    "#### Pipeline attributes\n",
    "- nlp.pipe_names : list of pipeline component names\n",
    "- nlp.pipeline : list of (name, component) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x25c1ba74308>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x25c1a6fffa8>),\n",
       " ('ner', <spacy.pipeline.pipes.EntityRecognizer at 0x25c1ba79048>)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Print the names of the pipeline components\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Print the full pipeline of (name, component) tuples\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Custom pipeline components\n",
    "\n",
    "This let you add your own function to the spaCy pipeline that is executed qhen you call the nlp object on a text\n",
    "\n",
    "#### Why custom components?\n",
    "    \n",
    "- Make a function execute automatically when you call nlp\n",
    "- Add your own metadata to documents and tokens\n",
    "- Updating built-in attributes like doc.ents\n",
    "\n",
    "#### Anatomy of a component\n",
    "\n",
    "- Function that takes a doc , modies it and returns it\n",
    "- Can be added using the nlp.add_pipe method\n",
    "\n",
    "|Argument |Description |Example|\n",
    "|---|---|---|\n",
    "|last |If True , add last| nlp.add_pipe(component, last=True)|\n",
    "|first |If True , add first| nlp.add_pipe(component, first=True)|\n",
    "|before |Add before component| nlp.add_pipe(component, before='ner')|\n",
    "|after |Add after component| nlp.add_pipe(component, after='tagger') |   \n",
    "\n",
    "Example of a simple component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom component\n",
    "def length_component(doc):\n",
    "    # Get the doc's length\n",
    "    doc_length = len(doc)\n",
    "    print(\"This document is {} tokens long.\".format(doc_length))\n",
    "    # Return the doc\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "  \n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(length_component, first=True)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 4 tokens long.\n"
     ]
    }
   ],
   "source": [
    "# Process a text\n",
    "doc = nlp('This is a sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complex components\n",
    "Using PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 1 tokens long.\n",
      "This document is 1 tokens long.\n",
      "This document is 2 tokens long.\n",
      "This document is 1 tokens long.\n",
      "This document is 1 tokens long.\n",
      "This document is 1 tokens long.\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "ANIMALS=['cat', 'dog', 'Golden Retriever', 'turtle', 'tortoise','rabbit']\n",
    "\n",
    "patterns = list(nlp.pipe(ANIMALS))\n",
    "matcher.add('ANIMALS', None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tagger', 'parser', 'ner', 'animal_component']\n"
     ]
    }
   ],
   "source": [
    "# Define the custom component\n",
    "def animal_component(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    spans = [Span(doc, start, end, label='ANIMAL')\n",
    "             for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the 'ner' component \n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 8 tokens long.\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Extension attributes\n",
    "    \n",
    "- Add custom metadata to documents,tokens and spans\n",
    "- Accessible via the ._ property\n",
    "- registered on the global Doc , Token or Span using the set_extension method\n",
    "    \n",
    "#### Extension attribute types\n",
    "    \n",
    "1. Attribute extensions\n",
    "    - Set a default value that can be overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 5 tokens long.\n",
      "[('I', False), ('live', False), ('in', False), ('Spain', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "# Import global classes\n",
    "from spacy.tokens import Doc, Token, Span\n",
    "\n",
    "# Register the Token extension attribute 'is_country' with the default value False\n",
    "Token.set_extension('is_country', default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Property extensions\n",
    "    - Work like properties in Python\n",
    "    - Define a getter and an optional setter function\n",
    "    - Getter only called when you retrieve the attribute value\n",
    "    - Span extensions should almost always use a getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 9 tokens long.\n",
      "reversed: llA\n",
      "reversed: snoitazilareneg\n",
      "reversed: era\n",
      "reversed: eslaf\n",
      "reversed: ,\n",
      "reversed: gnidulcni\n",
      "reversed: siht\n",
      "reversed: eno\n",
      "reversed: .\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "  \n",
    "# Register the Token property extension 'reversed' with the getter get_reversed\n",
    "Token.set_extension('reversed', getter=get_reversed)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print('reversed:', token._.reversed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 9 tokens long.\n",
      "has_number: True\n"
     ]
    }
   ],
   "source": [
    "# Define the getter function\n",
    "def get_has_number(doc):\n",
    "    # Return if any of the tokens in the doc return True for token.like_num\n",
    "    return any(token.like_num for token in doc)\n",
    "\n",
    "# Register the Doc property extension 'has_number' with the getter get_has_number\n",
    "Doc.set_extension('has_number', getter=get_has_number)\n",
    "\n",
    "# Process the text and check the custom has_number attribute \n",
    "doc = nlp(\"The museum closed for five years in 2012.\")\n",
    "print('has_number:', doc._.has_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Method extensions\n",
    "    - Makes a extension attribute callable\n",
    "    - Assign a function that becomes available as an object method\n",
    "    - Lets you pass arguments to the extension function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is 8 tokens long.\n",
      "<strong>Hello world</strong>\n"
     ]
    }
   ],
   "source": [
    "# Define the method\n",
    "def to_html(span, tag):\n",
    "    # Wrap the span text in a HTML tag and return it\n",
    "    return '<{tag}>{text}</{tag}>'.format(tag=tag, text=span.text)\n",
    "\n",
    "# Register the Span property extension 'to_html' with the method to_html\n",
    "Span.set_extension('to_html', method=to_html)\n",
    "\n",
    "# Process the text and call the to_html method on the span with the tag name 'strong'\n",
    "doc = nlp(\"Hello world, this is a sentence.\")\n",
    "span = doc[0:2]\n",
    "print(span._.to_html('strong'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entities and extensions\n",
    "Combine custom extension attributes with the model's predictions and create an attribute getter that returns a Wikipedia search URL if the span is a person, organization, or location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifty years None\n",
      "first None\n",
      "David Bowie https://en.wikipedia.org/w/index.php?search=David_Bowie\n"
     ]
    }
   ],
   "source": [
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in ('PERSON', 'ORG', 'GPE', 'LOCATION'):\n",
    "        entity_text = span.text.replace(' ', '_')\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using get getter get_wikipedia_url\n",
    "Span.set_extension('wikipedia_url', getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\"In over fifty years from his very first recordings right through to his last album, David Bowie was at the vanguard of contemporary culture.\")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Components with extensions\n",
    "Extension attributes are especially powerful if they're combined with custom pipeline components. In this exercise, you'll write a pipeline component that finds country names and a custom extension attribute that returns a country's capital, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRIES=['Afghanistan', 'Åland Islands', 'Albania', 'Algeria', 'American Samoa', 'Andorra', 'Angola', 'Anguilla', 'Antarctica', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bermuda', 'Bhutan', 'Bolivia (Plurinational State of)', 'Bonaire, Sint Eustatius and Saba', 'Bosnia and Herzegovina', 'Botswana', 'Bouvet Island', 'Brazil', 'British Indian Ocean Territory', 'United States Minor Outlying Islands', 'Virgin Islands (British)', 'Virgin Islands (U.S.)', 'Brunei Darussalam', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cambodia', 'Cameroon', 'Canada', 'Cabo Verde', 'Cayman Islands', 'Central African Republic', 'Chad', 'Chile', 'China', 'Christmas Island', 'Cocos (Keeling) Islands', 'Colombia', 'Comoros', 'Congo', 'Congo (Democratic Republic of the)', 'Cook Islands', 'Costa Rica', 'Croatia', 'Cuba', 'Curaçao', 'Cyprus', 'Czech Republic', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Ethiopia', 'Falkland Islands (Malvinas)', 'Faroe Islands', 'Fiji', 'Finland', 'France', 'French Guiana', 'French Polynesia', 'French Southern Territories', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Gibraltar', 'Greece', 'Greenland', 'Grenada', 'Guadeloupe', 'Guam', 'Guatemala', 'Guernsey', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Heard Island and McDonald Islands', 'Holy See', 'Honduras', 'Hong Kong', 'Hungary', 'Iceland', 'India', 'Indonesia', \"Côte d'Ivoire\", 'Iran (Islamic Republic of)', 'Iraq', 'Ireland', 'Isle of Man', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kuwait', 'Kyrgyzstan', \"Lao People's Democratic Republic\", 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Macao', 'Macedonia (the former Yugoslav Republic of)', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Martinique', 'Mauritania', 'Mauritius', 'Mayotte', 'Mexico', 'Micronesia (Federated States of)', 'Moldova (Republic of)', 'Monaco', 'Mongolia', 'Montenegro', 'Montserrat', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Caledonia', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'Niue', 'Norfolk Island', \"Korea (Democratic People's Republic of)\", 'Northern Mariana Islands', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine, State of', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Pitcairn', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Republic of Kosovo', 'Réunion', 'Romania', 'Russian Federation', 'Rwanda', 'Saint Barthélemy', 'Saint Helena, Ascension and Tristan da Cunha', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Martin (French part)', 'Saint Pierre and Miquelon', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Sint Maarten (Dutch part)', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Georgia and the South Sandwich Islands', 'Korea (Republic of)', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Svalbard and Jan Mayen', 'Swaziland', 'Sweden', 'Switzerland', 'Syrian Arab Republic', 'Taiwan', 'Tajikistan', 'Tanzania, United Republic of', 'Thailand', 'Timor-Leste', 'Togo', 'Tokelau', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Turks and Caicos Islands', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Venezuela (Bolivarian Republic of)', 'Viet Nam', 'Wallis and Futuna', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prague'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "##using capitals which was copy on a json file\n",
    "with open('datasets/capitals.json') as json_file:\n",
    "    capitals=json.load(json_file)\n",
    "capitals['Czech Republic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add('COUNTRIES', None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'countries_component']\n"
     ]
    }
   ],
   "source": [
    "def countries_component(doc):\n",
    "    # Create an entity Span with the label 'GPE' for all matches\n",
    "    doc.ents = [Span(doc, start, end, label='GPE')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(countries_component)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Czech Republic', 'GPE', 'Prague'), ('Slovakia', 'GPE', 'Bratislava')]\n"
     ]
    }
   ],
   "source": [
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: capitals.get(span.text)\n",
    "\n",
    "\n",
    "# Register the Span extension attribute 'capital' with the getter get_capital \n",
    "Span.set_extension('capital',getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Scaling and performance\n",
    "    \n",
    "#### Processing large volumes oftext\n",
    "- Use nlp.pipe method\n",
    "- Processes texts as a stream, yields Doc objects\n",
    "- Much faster than calling nlp on each text because it batches up the text\n",
    "\n",
    "    BAD:\n",
    ">docs = [nlp(text) for text in LOTS_OF_TEXTS]\n",
    "\n",
    "    GOOD:\n",
    ">docs = list(nlp.pipe(LOTS_OF_TEXTS))\n",
    "    \n",
    "#### Passing in context\n",
    "- Setting as_tuples=True on nlp.pipe lets you pass in (text, context) tuples\n",
    "- Yields (doc, context) tuples\n",
    "- Useful for associating metadata with the doc\n",
    "    \n",
    "#### Using only the tokenizer\n",
    "- don't run the whole pipeline!\n",
    "- Use nlp.make_doc to turn a text in to a Doc object\n",
    "\n",
    "    BAD:\n",
    ">doc = nlp(\"Hello world\")\n",
    "\n",
    "    GOOD:\n",
    ">doc = nlp.make_doc(\"Hello world!\")\n",
    "     \n",
    "#### Disabling pipeline components\n",
    "- Use nlp.disable_pipes to temporarily disable one or more pipes\n",
    "    \n",
    "```\n",
    "    # Disable tagger and parser\n",
    "    with nlp.disable_pipes('tagger','parser'):\n",
    "    # Process the text and print the entities\n",
    "    doc = nlp(text)\n",
    "    print(doc.ents)\n",
    "```    \n",
    "    \n",
    "- restores them after the with block\n",
    "- only runs the remaining components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS=['McDonalds is my favorite restaurant.',\n",
    " 'Here I thought @McDonalds only had precooked burgers but it seems they only have not cooked ones?? I have no time to get sick..',\n",
    " 'People really still eat McDonalds :(',\n",
    " 'The McDonalds in Spain has chicken wings. My heart is so happy ',\n",
    " '@McDonalds Please bring back the most delicious fast food sandwich of all times!!....The Arch Deluxe :P',\n",
    " 'please hurry and open. I WANT A #McRib SANDWICH SO BAD! :D',\n",
    " 'This morning i made a terrible decision by gettin mcdonalds and now my stomach is payin for it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos sin USAR NLP.PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD PERFORMANCE\n",
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['BAD']\n",
      "['terrible', 'payin']\n"
     ]
    }
   ],
   "source": [
    "print('BAD PERFORMANCE')\n",
    "# Process the texts and print the adjectives\n",
    "for text in TWEETS:\n",
    "    doc = nlp(text)\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) (@McDonalds,) (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) () ()\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "docs = [nlp(text) for text in TWEETS]\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[David Bowie, Angela Merkel, Lady Gaga]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = [nlp(person) for person in people]\n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejemplos USANDO NLP.PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEAN PERFORMANCE\n",
      "['favorite']\n",
      "['sick']\n",
      "[]\n",
      "['happy']\n",
      "['delicious', 'fast']\n",
      "['BAD']\n",
      "['terrible', 'payin']\n"
     ]
    }
   ],
   "source": [
    "print('CLEAN PERFORMANCE')\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(TWEETS):\n",
    "    print([token.text for token in doc if token.pos_ == 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(McDonalds,) (@McDonalds,) (McDonalds,) (McDonalds, Spain) (The Arch Deluxe,) () ()\n"
     ]
    }
   ],
   "source": [
    "# Process the texts and print the entities\n",
    "docs = list(nlp.pipe(TWEETS))\n",
    "entities = [doc.ents for doc in docs]\n",
    "print(*entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[David Bowie, Angela Merkel, Lady Gaga]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = ['David Bowie', 'Angela Merkel', 'Lady Gaga']\n",
    "\n",
    "# Create a list of patterns for the PhraseMatcher\n",
    "patterns = list(nlp.pipe(people))\n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing data with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA=[('One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.',\n",
    "  {'author': 'Franz Kafka', 'book': 'Metamorphosis'}),\n",
    " (\"I know not all that may be coming, but be it what it will, I'll go to it laughing.\",\n",
    "  {'author': 'Herman Melville', 'book': 'Moby-Dick or, The Whale'}),\n",
    " ('It was the best of times, it was the worst of times.',\n",
    "  {'author': 'Charles Dickens', 'book': 'A Tale of Two Cities'}),\n",
    " ('The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars.',\n",
    "  {'author': 'Jack Kerouac', 'book': 'On the Road'}),\n",
    " ('It was a bright cold day in April, and the clocks were striking thirteen.',\n",
    "  {'author': 'George Orwell', 'book': '1984'}),\n",
    " ('Nowadays people know the price of everything and the value of nothing.',\n",
    "  {'author': 'Oscar Wilde', 'book': 'The Picture Of Dorian Gray'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. \n",
      " — 'Metamorphosis' by Franz Kafka \n",
      "\n",
      "I know not all that may be coming, but be it what it will, I'll go to it laughing. \n",
      " — 'Moby-Dick or, The Whale' by Herman Melville \n",
      "\n",
      "It was the best of times, it was the worst of times. \n",
      " — 'A Tale of Two Cities' by Charles Dickens \n",
      "\n",
      "The only people for me are the mad ones, the ones who are mad to live, mad to talk, mad to be saved, desirous of everything at the same time, the ones who never yawn or say a commonplace thing, but burn, burn, burn like fabulous yellow roman candles exploding like spiders across the stars. \n",
      " — 'On the Road' by Jack Kerouac \n",
      "\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. \n",
      " — '1984' by George Orwell \n",
      "\n",
      "Nowadays people know the price of everything and the value of nothing. \n",
      " — 'The Picture Of Dorian Gray' by Oscar Wilde \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class and register the extensions 'author' and 'book'\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Register the Doc extension 'author' (default None)\n",
    "Doc.set_extension('author',default=None)\n",
    "\n",
    "# Register the Doc extension 'book' (default None)\n",
    "Doc.set_extension('book',default=None)\n",
    "\n",
    "for doc, context in nlp.pipe(DATA,as_tuples=True):\n",
    "    # Set the doc._.book and doc._.author attributes from the context\n",
    "    doc._.book = context['book']\n",
    "    doc._.author = context['author']\n",
    "    \n",
    "    # Print the text and custom attribute data\n",
    "    print(doc.text, '\\n', \"— '{}' by {}\".format(doc._.book, doc._.author), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective processing\n",
    "Using the nlp.make_doc and nlp.disable_pipes methods to only run selected components when processing a text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chick', '-', 'fil', '-', 'A', 'is', 'an', 'American', 'fast', 'food', 'restaurant', 'chain', 'headquartered', 'in', 'the', 'city', 'of', 'College', 'Park', ',', 'Georgia', ',', 'specializing', 'in', 'chicken', 'sandwiches', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Chick-fil-A is an American fast food restaurant chain headquartered in the city of College Park, Georgia, specializing in chicken sandwiches.\"\n",
    "\n",
    "# Only tokenize the text\n",
    "doc = nlp.make_doc(text)\n",
    "\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Chick-fil-A, American, College Park, Georgia)\n"
     ]
    }
   ],
   "source": [
    "# Disable the tagger and parser\n",
    "with nlp.disable_pipes('tagger','parser'):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    # Print the entities in the doc\n",
    "    print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_text",
   "language": "python",
   "name": "ml_text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
